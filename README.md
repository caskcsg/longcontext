# Long-context Pre-Training of Large Language Models

This folder contains the codes and models for our research papers on LongContext Post-Training.


- ðŸŽ‰ [Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model](https://arxiv.org/pdf/2405.19846), accepted as **ICLR 2025** conference paper ! 

- ðŸŽ‰ [NExtLong: Toward Effective Long-Context Training without Long Documents](https://arxiv.org/pdf/2501.12766), ranked 1st among LLMs under 10B on the [LongBench v2 leaderboard](https://longbench2.github.io/#leaderboard) (2025/01/23) and accepted as **ICML 2025** conference paper ! 

- ðŸŽ‰ [LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs](https://arxiv.org/pdf/2509.15568), accepted as **AAAI 2026** conference paper !

- ðŸ‘€ [EntropyLong: Effective Long-Context Training via Predictive Uncertainty](https://arxiv.org/pdf/2510.02330), under review .

# Long-context Instruct-Tuning of Large Language Models

- ðŸŽ‰ [LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions](https://arxiv.org/abs/2505.17134), accepted as **NeurIPS 2025** conference paper !

# Long-context Evaluation of Large Language Models

- ðŸ”¥ [LongBench Pro: A More Realistic and Comprehensive Bilingual Long-Context Evaluation Benchmark](https://arxiv.org/abs/2601.02872), under review.
