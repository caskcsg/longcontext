import json
import time
import logging
from tqdm import tqdm
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from modules.utils import *
from sentence_transformers import SentenceTransformer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# silence SentenceTransformer's logger
logging.getLogger("sentence_transformers").setLevel(logging.WARNING)

class Evaluator:
    """evaluator"""

    def __init__(self, evaluation_samples_num: int, embedding_model_path: str = "model/Qwen3-Embedding-8B") -> None:        
        self.embedding_model: SentenceTransformer = SentenceTransformer(
            embedding_model_path,
            tokenizer_kwargs={"padding_side": "left"},
        )
        self.evaluation_samples_num: int = evaluation_samples_num # use this value to check the length of evaluation data

        self.task_metric_config: Dict[str, str] = {
            "T1.1 Global Cohesive Retrieval": "NDCG",
            "T1.2 Key-Snippet Retrieval": "NDCG",
            "T2.1 Global Timeline Reconstruction": "Pairwise_Accuracy",
            "T2.2 Local Causal Chain Sorting": "Pairwise_Accuracy",
            "T3.1 Multi-Doc Integration QA": "Accuracy",
            "T3.2 Single-Hop Fact QA": "Accuracy",
            "T4.1 Global-Coverage Constrained Summary": "Summary",
            "T4.2 Query-Focused Summary": "Summary",
            "T5.1 Full-Sentence Citation Alignment": "F1_Score",
            "T5.2 Key-Statement Citation Alignment": "F1_Score",
            "T6.1 Large-Scale Document Clustering": "SubEM",
            "T6.2 Targeted Subset Cluster Identification": "F1_Score",
            "T6.3 Global Frequency Analysis": "Pairwise_Accuracy",
            "T7.1 Global Conflict & Inconsistency Localization": "F1_Score",
            "T7.2 Targeted Rule or Condition Violation Detection": "F1_Score",
            "T7.3 Comprehensive Error & Anomaly Sweep": "F1_Score",
            "T8.1 Structured Multi-Source Consistency Verification": "SubEM",
            "T8.2 Single-Source Targeted Aggregation": "SubEM",
            "T8.3 Long-Context Procedural State Tracking": "SubEM",
            "T9.1 Dependency-Aware Multi-Version Impact Analysis": "F1_Score",
            "T9.2 Localized Interface Change Detection": "F1_Score",
            "T10.1 Large-Scale In-Context Rule Induction": "SubEM",
            "T10.2 Targeted Example-Based Rule Induction": "SubEM",
            "T11.1 Long-Range Entity & Commitment Tracking": "Accuracy",
            "T11.2 Short-Range Reference Resolution & State Query": "Accuracy"
        }

        self.evaluate_configs: Dict[str, List[str]] = {
            "token_length": ["8k", "16k", "32k", "64k", "128k", "256k"],
            "contextual_requirement": ["Full", "Partial"],
            "difficulty": ["Easy", "Moderate", "Hard", "Extreme"],
            "primary_task": [
                "T1. Retrieval & Ranking",
                "T2. Sequencing & Structure Reconstruction",
                "T3. Evidence-Grounded QA",
                "T4. Summarization & Synthesis",
                "T5. Attribution & Citation Alignment",
                "T6. Aggregation & Clustering",
                "T7. Consistency & Compliance Checking",
                "T8. Structured & Numeric Reasoning",
                "T9. Version & Code Diff Analysis",
                "T10. Rule Induction & In-Context Learning",
                "T11. Dialogue Memory & Long-Horizon Tracking"
            ],
            "language": ["Chinese", "English"]
        }
    
    def load_jsonl_file(self, file_path: str) -> List[Any]:
        data: List[Any] = []
        with open(file_path, 'r', encoding='utf-8') as file:
            for line in file:
                data.append(json.loads(line))
        return data

    def load_jsonl_files(self, file_paths: List[str]) -> List[Any]:
        all_data: List[Any] = []
        for file_path in file_paths:
            data: List[Any] = self.load_jsonl_file(file_path)
            all_data.extend(data)
        return all_data
    
    def save_json_file(self, data: List[Any], file_path: str) -> None:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

    def calculate_metric(self, secondary_task: str, answer: List[str], prediction: str, is_zh: bool) -> Tuple[bool, float]:
        """
        calculate metric for single item
        """
        try:
            if prediction == "":
                return False, 0.0
            
            metric_name: str = self.task_metric_config[secondary_task]
            
            if metric_name == "NDCG":
                metric_value = NDCG(answer, prediction)
            elif metric_name == "Pairwise_Accuracy":
                metric_value = Pairwise_Accuracy(answer, prediction)
            elif metric_name == "Accuracy":
                metric_value = Accuracy(answer, prediction)
            elif metric_name == "F1_Score":
                metric_value = F1_Score(answer, prediction)
            elif metric_name == "SubEM":
                metric_value = SubEM(answer, prediction)
            elif metric_name == "Summary":
                metric_value = Summary(self.embedding_model, answer, prediction, is_zh)
            else:
                logger.warning(f"Unknown metric: {metric_name}")
                return False, 0.0
                
            # validate metric value range
            assert 0.0 <= metric_value <= 1.0, f"Metric {metric_value} is not in [0, 1]"
            return True, metric_value
            
        except Exception as e:
            logger.error(f"Error calculating metric for {secondary_task}: {e}")
            return False, 0.0

    def evaluate_single_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """evaluate single item"""
        secondary_task: str = item["secondary_task"]
        is_zh: bool = True if item["language"] == "Chinese" else False
        answer: List[str] = item["answer"]
        key_words: Optional[List[str]] = item.get("key_words", None)
        prediction: str = item["prediction"]
        
        # calculate metric
        success, metric_value = self.calculate_metric(secondary_task, answer, prediction, is_zh)
        item["metric"] = metric_value
        
        return success, item

    def evaluate(self, inference_files_in_shard_group: List[str], evaluation_file: str) -> None:
        """sequential evaluation"""
        logger.info(f"load data from {inference_files_in_shard_group}")
        data: List[Any] = self.load_jsonl_files(inference_files_in_shard_group)

        # check data length
        assert len(data) == self.evaluation_samples_num, f"inference datas num {len(data)} != evaluation samples num {self.evaluation_samples_num}, please infer again!"
        
        # clear output file
        with open(evaluation_file, 'w', encoding='utf-8') as f:
            pass
        
        # sequential evaluation
        fail_samples_num = 0 # number of samples with failed inference or evaluation
        logger.info(f"start sequential evaluation of {len(data)} items")
        for item in tqdm(data, desc="evaluation progress"):
            success, result = self.evaluate_single_item(item)
            if not success:
                fail_samples_num += 1
            # write result to file
            with open(evaluation_file, 'a', encoding='utf-8') as fout:
                fout.write(json.dumps(result, ensure_ascii=False) + '\n')
                fout.flush()
        
        logger.info(f"evaluation completed, results saved to: {evaluation_file}")
        if fail_samples_num > 0:
            logger.warning(f"there are {fail_samples_num} samples with failed inference or evaluation, please check them carefully!")
        return fail_samples_num

    def metric_summary(self, evaluation_file: str, fail_samples_num: int, summary_file: str, bon_num: int = 3) -> None:
        """summary metrics"""
        logger.info(f"summary metrics from {evaluation_file}")
        data: List[Any] = self.load_jsonl_file(evaluation_file)

        summary_results: Dict[str, Any] = {}
        summary_results['date'] = datetime.now().strftime("%Y-%m-%d")

        # calculate average overall metrics for all inference iterations
        logger.info(f"calculate average overall metrics for all inference iterations")
        average_overall_results, inference_inconsistent_samples_num = get_average_overall_results(data, bon_num)
        if inference_inconsistent_samples_num > 0:
            logger.warning(f"there are {inference_inconsistent_samples_num} samples with inconsistent inferences, their inference number is not equal to {bon_num}, check them carefully!")
        
        summary_results['total_questions_num'] = len(average_overall_results)
        summary_results['inference_iterations'] = bon_num
        summary_results['total_samples_num'] = len(data)
        summary_results['fail_samples_num'] = fail_samples_num
        summary_results['inference_inconsistent_samples_num'] = inference_inconsistent_samples_num
        summary_results['average_overall_metric'] = calculate_overall_metrics(average_overall_results)

        # calculate overall metrics for each inference iteration
        for inference_iteration_idx in range(1, bon_num + 1):
            logger.info(f"calculate overall metric for inference iteration {inference_iteration_idx}")
            summary_results[f'inference_iteration_{inference_iteration_idx}_overall_metric'] = calculate_overall_metrics(get_inference_iteration_idx_results(data, inference_iteration_idx))

        # calculate average metrics for each dimension
        for dimension, sort_keys in self.evaluate_configs.items():
            summary_results[f'average_{dimension}_metric'] = calculate_dimension_metrics(average_overall_results, dimension, sort_keys)

        # cycle calculate best-of-n & pass@n metrics
        for i in range(1, bon_num + 1):
            logger.info(f"calculate metrics for BoN-{i} & pass@{i} metrics")
            summary_results_bon_i: Dict[str, Any] = {}

            # one question may have multiple results, get the best result of BoN-i for each question
            best_of_n_results = get_best_of_n_results(data, i)
            summary_results_bon_i['overall_metric'] = calculate_overall_metrics(best_of_n_results)
            for dimension, sort_keys in self.evaluate_configs.items():
                summary_results_bon_i[dimension] = calculate_dimension_metrics(best_of_n_results, dimension, sort_keys)

            summary_results['BoN-' + str(i)] = summary_results_bon_i
            summary_results['pass@' + str(i)] = calculate_pass_n_metrics(best_of_n_results)
            
        # save metrics
        self.save_json_file(summary_results, summary_file)
        logger.info(f"metrics summary saved to: {summary_file}")